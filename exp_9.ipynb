{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed27566-8791-4ca2-973d-580796444f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CBIT\\AppData\\Local\\Temp\\ipykernel_13196\\1294496389.py:14: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 6)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             150 non-null    int64  \n",
      " 1   SepalLengthCm  150 non-null    float64\n",
      " 2   SepalWidthCm   150 non-null    float64\n",
      " 3   PetalLengthCm  150 non-null    float64\n",
      " 4   PetalWidthCm   150 non-null    float64\n",
      " 5   Species        150 non-null    object \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 7.2+ KB\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "y-intercept             : -0.024298523519847626\n",
      "beta coefficients       : [ 0.00680677 -0.10726764 -0.00624275  0.22428158  0.27196685]\n",
      "Mean Abs Error MAE      : 0.14966835490524968\n",
      "Mean Sqrt Error MSE     : 0.032554517379698125\n",
      "Root Mean Sqrt Error RMSE: 0.18042870442282216\n",
      "r2 value                : 0.9446026069799255\n",
      "Decision Tree\n",
      "==============================\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        16\n",
      "Iris-versicolor       0.95      1.00      0.97        18\n",
      " Iris-virginica       1.00      0.91      0.95        11\n",
      "\n",
      "       accuracy                           0.98        45\n",
      "      macro avg       0.98      0.97      0.98        45\n",
      "   weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  1 10]]\n",
      "accuracy is 0.9777777777777777\n",
      "Random Forest Classifier\n",
      "===================================\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        16\n",
      "Iris-versicolor       1.00      1.00      1.00        18\n",
      " Iris-virginica       1.00      1.00      1.00        11\n",
      "\n",
      "       accuracy                           1.00        45\n",
      "      macro avg       1.00      1.00      1.00        45\n",
      "   weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  0 11]]\n",
      "accuracy is  1.0\n",
      "LogisticRegression\n",
      "===================================\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        16\n",
      "Iris-versicolor       1.00      1.00      1.00        18\n",
      " Iris-virginica       1.00      1.00      1.00        11\n",
      "\n",
      "       accuracy                           1.00        45\n",
      "      macro avg       1.00      1.00      1.00        45\n",
      "   weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  0 11]]\n",
      "accuracy is 1.0\n",
      "K-Nearest Neighbours\n",
      "===================================\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        16\n",
      "Iris-versicolor       1.00      1.00      1.00        18\n",
      " Iris-virginica       1.00      1.00      1.00        11\n",
      "\n",
      "       accuracy                           1.00        45\n",
      "      macro avg       1.00      1.00      1.00        45\n",
      "   weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  0 11]]\n",
      "accuracy is 1.0\n",
      "Naive Bayes\n",
      "===================================\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        16\n",
      "Iris-versicolor       1.00      1.00      1.00        18\n",
      " Iris-virginica       1.00      1.00      1.00        11\n",
      "\n",
      "       accuracy                           1.00        45\n",
      "      macro avg       1.00      1.00      1.00        45\n",
      "   weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  0 11]]\n",
      "accuracy is 1.0\n",
      "Support Vector Machine\n",
      "===================================\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        16\n",
      "Iris-versicolor       1.00      1.00      1.00        18\n",
      " Iris-virginica       1.00      1.00      1.00        11\n",
      "\n",
      "       accuracy                           1.00        45\n",
      "      macro avg       1.00      1.00      1.00        45\n",
      "   weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  0 11]]\n",
      "accuracy is 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CBIT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "import pylab as pl\n",
    "\n",
    "# Display HTML\n",
    "from IPython.core.display import display, HTML\n",
    "data =  pd.read_csv('Iris.csv')\n",
    "data.head()\n",
    "print(data.shape)\n",
    "data.info()\n",
    "X = data.iloc[:, :-1].values    #   X -> Feature Variables\n",
    "y = data.iloc[:, -1].values #   y ->  Target\n",
    "# Splitting the data into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "XL = data.iloc[:, :-1].values    #   X -> Feature Variables\n",
    "yL = data.iloc[:, -1].values #   y ->  Target\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y_train= le.fit_transform(yL)\n",
    "\n",
    "print(Y_train)\n",
    "# This is only for Linear Regretion \n",
    "X_trainL, X_testL, y_trainL, y_testL = train_test_split(XL, Y_train, test_size = 0.3, random_state = 0)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "modelLR = LinearRegression()\n",
    "modelLR.fit(X_trainL, y_trainL)\n",
    "\n",
    "Y_pred = modelLR.predict(X_testL)\n",
    "from sklearn import metrics\n",
    "#calculating the residuals\n",
    "print('y-intercept             :' , modelLR.intercept_)\n",
    "print('beta coefficients       :' , modelLR.coef_)\n",
    "print('Mean Abs Error MAE      :' ,metrics.mean_absolute_error(y_testL,Y_pred))\n",
    "print('Mean Sqrt Error MSE     :' ,metrics.mean_squared_error(y_testL,Y_pred))\n",
    "print('Root Mean Sqrt Error RMSE:' ,np.sqrt(metrics.mean_squared_error(y_testL,Y_pred)))\n",
    "print('r2 value                :' ,metrics.r2_score(y_testL,Y_pred))\n",
    "# Decision Tree's\n",
    "print(\"Decision Tree\\n==============================\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Model = DecisionTreeClassifier()\n",
    "Model.fit(X_train, y_train)\n",
    "y_pred = Model.predict(X_test)\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))\n",
    "# RandomForestClassifier\n",
    "print(\"Random Forest Classifier\\n===================================\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Model=RandomForestClassifier(max_depth=2)\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))\n",
    "# LogisticRegression\n",
    "print(\"LogisticRegression\\n===================================\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "Model = LogisticRegression()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))\n",
    "# K-Nearest Neighbours\n",
    "print(\"K-Nearest Neighbours\\n===================================\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "Model = KNeighborsClassifier(n_neighbors=8)\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))\n",
    "# Naive Bayes\n",
    "print(\"Naive Bayes\\n===================================\")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "Model = GaussianNB()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))\n",
    "# Support Vector Machine\n",
    "print(\"Support Vector Machine\\n===================================\")\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "Model = SVC()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f9ed4-2cbb-4217-883c-484014b7f5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
